{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-04-23T11:53:22.807874Z","iopub.status.busy":"2024-04-23T11:53:22.807564Z","iopub.status.idle":"2024-04-23T11:53:23.952145Z","shell.execute_reply":"2024-04-23T11:53:23.951190Z","shell.execute_reply.started":"2024-04-23T11:53:22.807848Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["/kaggle/input/Entertainment_Music/model_outputs/informal.nmt_copy\n","/kaggle/input/Entertainment_Music/model_outputs/formal.nmt_baseline\n","/kaggle/input/Entertainment_Music/model_outputs/formal.nmt_combined\n","/kaggle/input/Entertainment_Music/model_outputs/informal.nmt_combined\n","/kaggle/input/Entertainment_Music/model_outputs/informal.rule_based\n","/kaggle/input/Entertainment_Music/model_outputs/informal.pbmt\n","/kaggle/input/Entertainment_Music/model_outputs/formal.nmt_copy\n","/kaggle/input/Entertainment_Music/model_outputs/formal.pbmt\n","/kaggle/input/Entertainment_Music/model_outputs/informal.nmt_baseline\n","/kaggle/input/Entertainment_Music/model_outputs/formal.rule_based\n","/kaggle/input/Entertainment_Music/tune/formal\n","/kaggle/input/Entertainment_Music/tune/informal.ref2\n","/kaggle/input/Entertainment_Music/tune/formal.ref2\n","/kaggle/input/Entertainment_Music/tune/formal.ref3\n","/kaggle/input/Entertainment_Music/tune/formal.ref1\n","/kaggle/input/Entertainment_Music/tune/informal\n","/kaggle/input/Entertainment_Music/tune/formal.ref0\n","/kaggle/input/Entertainment_Music/tune/informal.ref3\n","/kaggle/input/Entertainment_Music/tune/informal.ref0\n","/kaggle/input/Entertainment_Music/tune/informal.ref1\n","/kaggle/input/Entertainment_Music/test/formal\n","/kaggle/input/Entertainment_Music/test/informal.ref2\n","/kaggle/input/Entertainment_Music/test/formal.ref2\n","/kaggle/input/Entertainment_Music/test/formal.ref3\n","/kaggle/input/Entertainment_Music/test/formal.ref1\n","/kaggle/input/Entertainment_Music/test/informal\n","/kaggle/input/Entertainment_Music/test/formal.ref0\n","/kaggle/input/Entertainment_Music/test/informal.ref3\n","/kaggle/input/Entertainment_Music/test/informal.ref0\n","/kaggle/input/Entertainment_Music/test/informal.ref1\n","/kaggle/input/Entertainment_Music/train/formal\n","/kaggle/input/Entertainment_Music/train/informal\n","/kaggle/input/Family_Relationships/model_outputs/informal.nmt_copy\n","/kaggle/input/Family_Relationships/model_outputs/formal.nmt_baseline\n","/kaggle/input/Family_Relationships/model_outputs/formal.nmt_combined\n","/kaggle/input/Family_Relationships/model_outputs/informal.nmt_combined\n","/kaggle/input/Family_Relationships/model_outputs/informal.rule_based\n","/kaggle/input/Family_Relationships/model_outputs/informal.pbmt\n","/kaggle/input/Family_Relationships/model_outputs/formal.nmt_copy\n","/kaggle/input/Family_Relationships/model_outputs/formal.pbmt\n","/kaggle/input/Family_Relationships/model_outputs/informal.nmt_baseline\n","/kaggle/input/Family_Relationships/model_outputs/formal.rule_based\n","/kaggle/input/Family_Relationships/tune/formal\n","/kaggle/input/Family_Relationships/tune/informal.ref2\n","/kaggle/input/Family_Relationships/tune/formal.ref2\n","/kaggle/input/Family_Relationships/tune/formal.ref3\n","/kaggle/input/Family_Relationships/tune/formal.ref1\n","/kaggle/input/Family_Relationships/tune/informal\n","/kaggle/input/Family_Relationships/tune/formal.ref0\n","/kaggle/input/Family_Relationships/tune/informal.ref3\n","/kaggle/input/Family_Relationships/tune/informal.ref0\n","/kaggle/input/Family_Relationships/tune/informal.ref1\n","/kaggle/input/Family_Relationships/test/formal\n","/kaggle/input/Family_Relationships/test/informal.ref2\n","/kaggle/input/Family_Relationships/test/formal.ref2\n","/kaggle/input/Family_Relationships/test/formal.ref3\n","/kaggle/input/Family_Relationships/test/formal.ref1\n","/kaggle/input/Family_Relationships/test/informal\n","/kaggle/input/Family_Relationships/test/formal.ref0\n","/kaggle/input/Family_Relationships/test/informal.ref3\n","/kaggle/input/Family_Relationships/test/informal.ref0\n","/kaggle/input/Family_Relationships/test/informal.ref1\n","/kaggle/input/Family_Relationships/train/formal\n","/kaggle/input/Family_Relationships/train/informal\n"]}],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-04-23T11:53:23.954248Z","iopub.status.busy":"2024-04-23T11:53:23.953881Z","iopub.status.idle":"2024-04-23T11:54:04.709766Z","shell.execute_reply":"2024-04-23T11:54:04.708569Z","shell.execute_reply.started":"2024-04-23T11:53:23.954223Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Found existing installation: transformers 4.39.3\n","Uninstalling transformers-4.39.3:\n","  Successfully uninstalled transformers-4.39.3\n","Collecting transformers\n","  Downloading transformers-4.40.0-py3-none-any.whl.metadata (137 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.6/137.6 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.22.2)\n","Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\n","Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\n","Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\n","Collecting tokenizers<0.20,>=0.19 (from transformers)\n","  Downloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n","Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.2)\n","Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.2.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\n","Downloading transformers-4.40.0-py3-none-any.whl (9.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.0/9.0 MB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m81.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n","\u001b[?25hInstalling collected packages: tokenizers, transformers\n","  Attempting uninstall: tokenizers\n","    Found existing installation: tokenizers 0.15.2\n","    Uninstalling tokenizers-0.15.2:\n","      Successfully uninstalled tokenizers-0.15.2\n","Successfully installed tokenizers-0.19.1 transformers-4.40.0\n","Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.28.0)\n","Collecting accelerate\n","  Downloading accelerate-0.29.3-py3-none-any.whl.metadata (18 kB)\n","Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (21.3)\n","Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\n","Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0.1)\n","Requirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.1.2)\n","Requirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.22.2)\n","Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.4.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.1.1)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.13.1)\n","Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (4.9.0)\n","Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\n","Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\n","Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\n","Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2024.2.0)\n","Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2.31.0)\n","Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (4.66.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (1.26.18)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\n","Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n","Downloading accelerate-0.29.3-py3-none-any.whl (297 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m297.6/297.6 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n","\u001b[?25hInstalling collected packages: accelerate\n","  Attempting uninstall: accelerate\n","    Found existing installation: accelerate 0.28.0\n","    Uninstalling accelerate-0.28.0:\n","      Successfully uninstalled accelerate-0.28.0\n","Successfully installed accelerate-0.29.3\n"]}],"source":["!pip uninstall -y transformers\n","!pip install transformers\n","!pip install accelerate -U"]},{"cell_type":"code","execution_count":63,"metadata":{"execution":{"iopub.execute_input":"2024-04-23T12:59:42.529300Z","iopub.status.busy":"2024-04-23T12:59:42.528321Z","iopub.status.idle":"2024-04-23T12:59:44.677644Z","shell.execute_reply":"2024-04-23T12:59:44.676505Z","shell.execute_reply.started":"2024-04-23T12:59:42.529265Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c94392e4e01f46d1a61e64c7d3f00041","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e38829978444485f8c49be96280173b4","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"70a7e0d7c8b246eca3f86f6cea45261e","version_major":2,"version_minor":0},"text/plain":["generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n","model_checkpoint = \"t5-small\"\n","model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"]},{"cell_type":"code","execution_count":64,"metadata":{"execution":{"iopub.execute_input":"2024-04-23T12:59:48.308697Z","iopub.status.busy":"2024-04-23T12:59:48.307753Z","iopub.status.idle":"2024-04-23T12:59:48.351030Z","shell.execute_reply":"2024-04-23T12:59:48.349965Z","shell.execute_reply.started":"2024-04-23T12:59:48.308662Z"},"trusted":true},"outputs":[],"source":["batch_size = 8\n","model_name = model_checkpoint.split(\"/\")[-1]\n","args = Seq2SeqTrainingArguments(\n","    f\"{model_name}-informal\",\n","    evaluation_strategy = \"epoch\",\n","    learning_rate=2e-5,\n","    per_device_train_batch_size=batch_size,\n","    per_device_eval_batch_size=batch_size,\n","    weight_decay=0.01,\n","    save_total_limit=3,\n","    num_train_epochs=5,\n","    predict_with_generate=True,\n","     fp16=True,\n","    push_to_hub=True,\n",")"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-04-23T11:54:32.578143Z","iopub.status.busy":"2024-04-23T11:54:32.577732Z","iopub.status.idle":"2024-04-23T11:54:33.329277Z","shell.execute_reply":"2024-04-23T11:54:33.328338Z","shell.execute_reply.started":"2024-04-23T11:54:32.578109Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /usr/share/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n"]},{"data":{"text/plain":["True"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["import nltk\n","nltk.download('punkt')\n","nltk.download('averaged_perceptron_tagger')"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-04-23T11:54:33.330835Z","iopub.status.busy":"2024-04-23T11:54:33.330486Z","iopub.status.idle":"2024-04-23T12:03:00.446835Z","shell.execute_reply":"2024-04-23T12:03:00.445820Z","shell.execute_reply.started":"2024-04-23T11:54:33.330788Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy: 0.8354299855640338\n","Confusion Matrix:\n","[[1665  436]\n"," [ 362 2386]]\n"]}],"source":["import os\n","import pandas as pd\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score, confusion_matrix\n","from sklearn.pipeline import Pipeline\n","from sklearn.preprocessing import StandardScaler\n","from nltk.stem import SnowballStemmer\n","from nltk.tokenize import word_tokenize\n","from nltk import pos_tag\n","from nltk.corpus import stopwords\n","\n","# Initialize Snowball stemmer\n","stemmer = SnowballStemmer(\"english\")\n","\n","# Initialize stopwords\n","stop_words = set(stopwords.words('english'))\n","\n","# Function to apply stemming to a sentence using Snowball stemmer\n","def apply_stemming(sentence):\n","    tokens = word_tokenize(sentence)\n","    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n","    return ' '.join(stemmed_tokens)\n","\n","# Function to replace terms with POS tags based on weights from the model\n","def replace_with_pos_tags(sentence, weights, is_formal):\n","    tokens = word_tokenize(sentence)\n","    stemmed_tokens = [stemmer.stem(token) for token in tokens]  # Stem tokens\n","    pos_tags = pos_tag(tokens)  # Use original tokens for POS tagging\n","    replaced_sentence = []\n","\n","    # Calculate N as described\n","    N = len(stemmed_tokens) // 4\n","\n","    # Filter out stop words and stem them\n","    tokens_without_stopwords = [stemmer.stem(token) for token in tokens if token.lower() not in stop_words]\n","\n","    # Sort stemmed tokens by weight\n","    sorted_tokens = sorted(tokens_without_stopwords, key=lambda x: abs(weights.get(x, 0)), reverse=True)\n","\n","    # Extract N tokens with highest absolute weights\n","    top_N_tokens = sorted_tokens[:N]\n","\n","    for token, tag in pos_tags:\n","        stemmed_token = stemmer.stem(token)\n","        if stemmed_token in top_N_tokens:\n","            weight = abs(weights.get(stemmed_token, 0))\n","            if (is_formal and weight >= 0.01) or (not is_formal and weight >= 0.02):\n","                replaced_sentence.append(tag)\n","            else:\n","                replaced_sentence.append(token)\n","        else:\n","            replaced_sentence.append(token)\n","\n","    return ' '.join(replaced_sentence)\n","\n","def read_data(file_path):\n","    with open(file_path, \"r\", encoding=\"latin-1\") as file:\n","        sentences = file.readlines()\n","    return [apply_stemming(sentence.strip()) for sentence in sentences]\n","\n","# Paths to data files\n","train_formal_path_1 = \"/kaggle/input/Entertainment_Music/train/formal\"\n","train_informal_path_1 = \"/kaggle/input/Entertainment_Music/train/informal\"\n","test_formal_path_1 = \"/kaggle/input/Entertainment_Music/test/formal\"\n","test_informal_path_1 = \"/kaggle/input/Entertainment_Music/test/informal\"\n","\n","train_formal_path_2 = \"/kaggle/input/Family_Relationships/train/formal\"\n","train_informal_path_2 = \"/kaggle/input/Family_Relationships/train/informal\"\n","test_formal_path_2 = \"/kaggle/input/Family_Relationships/test/formal\"\n","test_informal_path_2 = \"/kaggle/input/Family_Relationships/test/informal\"\n","\n","X_train = (read_data(train_formal_path_1) + read_data(train_formal_path_2) +\n","           read_data(train_informal_path_1) + read_data(train_informal_path_2))\n","Y_train = [0] * (len(read_data(train_formal_path_1)) + len(read_data(train_formal_path_2))) + \\\n","          [1] * (len(read_data(train_informal_path_1)) + len(read_data(train_informal_path_2)))  # Labels for training data\n","\n","X_test = read_data(test_formal_path_1) + read_data(test_formal_path_2) + \\\n","         read_data(test_informal_path_1) + read_data(test_informal_path_2)\n","Y_test = [0] * (len(read_data(test_formal_path_1)) + len(read_data(test_formal_path_2))) + \\\n","         [1] * (len(read_data(test_informal_path_1)) + len(read_data(test_informal_path_2)))  # Labels for test data\n","\n","\n","# Initialize TfidfVectorizer with n-grams\n","vectorizer = TfidfVectorizer(max_features=200000, ngram_range=(1, 3), min_df=1, max_df=0.85)\n","\n","\n","# Initialize StandardScaler for scaling the data\n","scaler = StandardScaler(with_mean=False)\n","\n","# Create a Pipeline for preprocessing and model training\n","pipeline = Pipeline([\n","    ('vectorizer', vectorizer),\n","    ('classifier', LogisticRegression( max_iter=5000))\n","])\n","\n","# Fit the pipeline on the training data\n","pipeline.fit(X_train, Y_train)\n","\n","# Extract feature names from the vectorizer\n","# feature_names = vectorizer.get_feature_names()\n","feature_names = vectorizer.get_feature_names_out()\n","\n","\n","# Extract coefficients (weights) from the logistic regression model\n","weights = dict(zip(feature_names, pipeline.named_steps['classifier'].coef_[0]))\n","\n","# Set threshold for replacing terms with POS tags\n","# threshold = 0.2\n","\n","# Replace terms with POS tags in the training data itself\n","X_train_transformed = [replace_with_pos_tags(X_train[i], weights, not(Y_train[i])) for i  in range(len(X_train))]\n","\n","# Print out the original and transformed sentences\n","# for original_sentence, transformed_sentence in zip(X_train, X_train_transformed):\n","#     print(\"Original Sentence:\", original_sentence)\n","#     print(\"Transformed Sentence:\", transformed_sentence)\n","#     print()  # Add a newline for clarity\n","\n","# Make predictions on the test data\n","predicted_probabilities = pipeline.predict_proba(X_test)\n","\n","# Define a new threshold\n","new_threshold = 0.42  # Example threshold, you can adjust it according to your needs\n","\n","# Modify predictions based on the new threshold\n","predicted_labels = (predicted_probabilities[:, 1] >= new_threshold).astype(int)\n","\n","# Calculate accuracy\n","accuracy = accuracy_score(Y_test, predicted_labels)\n","print(\"Accuracy:\", accuracy)\n","\n","# Compute confusion matrix\n","cm = confusion_matrix(Y_test, predicted_labels)\n","print(\"Confusion Matrix:\")\n","print(cm)\n"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-04-23T12:03:00.448630Z","iopub.status.busy":"2024-04-23T12:03:00.448326Z","iopub.status.idle":"2024-04-23T12:03:00.458280Z","shell.execute_reply":"2024-04-23T12:03:00.457252Z","shell.execute_reply.started":"2024-04-23T12:03:00.448603Z"},"trusted":true},"outputs":[],"source":["def replace_with_pos_tags(sentence, weights, is_formal):\n","    tokens = word_tokenize(sentence)\n","    stemmed_tokens = [stemmer.stem(token) for token in tokens]  # Stem tokens\n","    pos_tags = pos_tag(tokens)  # Use original tokens for POS tagging\n","    replaced_sentence = []\n","\n","    # Calculate N as described\n","    N = len(stemmed_tokens) // 4\n","\n","    # Filter out stop words and stem them\n","    tokens_without_stopwords = [stemmer.stem(token) for token in tokens if token.lower() not in stop_words]\n","\n","    # Sort stemmed tokens by weight\n","    sorted_tokens = sorted(tokens_without_stopwords, key=lambda x: abs(weights.get(x, 0)), reverse=True)\n","\n","    # Extract N tokens with highest absolute weights\n","    top_N_tokens = sorted_tokens[:N]\n","\n","    for token, tag in pos_tags:\n","        stemmed_token = stemmer.stem(token)\n","        if stemmed_token in top_N_tokens:\n","            weight = abs(weights.get(stemmed_token, 0))\n","            if (is_formal and weight >= 0.01) or (not is_formal and weight >= 0.02):\n","                replaced_sentence.append(tag)\n","            else:\n","                replaced_sentence.append(token)\n","        else:\n","            replaced_sentence.append(token)\n","\n","    return ' '.join(replaced_sentence)\n"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-04-23T12:03:00.459698Z","iopub.status.busy":"2024-04-23T12:03:00.459438Z","iopub.status.idle":"2024-04-23T12:03:00.482040Z","shell.execute_reply":"2024-04-23T12:03:00.481165Z","shell.execute_reply.started":"2024-04-23T12:03:00.459675Z"},"trusted":true},"outputs":[],"source":["def read_data(file_path):\n","    with open(file_path, \"r\", encoding=\"latin-1\") as file:\n","        sentences = file.readlines()\n","    return [sentence.strip() for sentence in sentences]"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-04-23T12:03:00.483322Z","iopub.status.busy":"2024-04-23T12:03:00.483051Z","iopub.status.idle":"2024-04-23T12:03:00.677543Z","shell.execute_reply":"2024-04-23T12:03:00.676653Z","shell.execute_reply.started":"2024-04-23T12:03:00.483300Z"},"trusted":true},"outputs":[],"source":["X_train = (read_data(train_formal_path_1) + read_data(train_formal_path_2) +\n","           read_data(train_informal_path_1) + read_data(train_informal_path_2))\n","Y_train = [0] * (len(read_data(train_formal_path_1)) + len(read_data(train_formal_path_2))) + \\\n","          [1] * (len(read_data(train_informal_path_1)) + len(read_data(train_informal_path_2)))  # Labels for training data\n","\n","X_test = read_data(test_formal_path_1) + read_data(test_formal_path_2) + \\\n","         read_data(test_informal_path_1) + read_data(test_informal_path_2)\n","Y_test = [0] * (len(read_data(test_formal_path_1)) + len(read_data(test_formal_path_2))) + \\\n","         [1] * (len(read_data(test_informal_path_1)) + len(read_data(test_informal_path_2)))"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-04-23T12:03:28.203011Z","iopub.status.busy":"2024-04-23T12:03:28.202627Z","iopub.status.idle":"2024-04-23T12:08:42.222498Z","shell.execute_reply":"2024-04-23T12:08:42.221598Z","shell.execute_reply.started":"2024-04-23T12:03:28.202982Z"},"trusted":true},"outputs":[],"source":["X_train_transformed = [replace_with_pos_tags(X_train[i], weights, not(Y_train[i])) for i  in range(len(X_train))]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-22T03:43:37.451800Z","iopub.status.busy":"2024-04-22T03:43:37.450910Z","iopub.status.idle":"2024-04-22T03:43:37.458898Z","shell.execute_reply":"2024-04-22T03:43:37.457936Z","shell.execute_reply.started":"2024-04-22T03:43:37.451764Z"},"trusted":true},"outputs":[],"source":["new=replace_with_pos_tags(\"There's nothing he needs to change.\", weights, True)\n","\n","new"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-04-23T12:03:24.227130Z","iopub.status.idle":"2024-04-23T12:03:24.227455Z","shell.execute_reply":"2024-04-23T12:03:24.227305Z","shell.execute_reply.started":"2024-04-23T12:03:24.227291Z"},"trusted":true},"outputs":[],"source":["X_test=[apply_stemming(sentence) for sentence in X_test]"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-04-23T12:08:53.772014Z","iopub.status.busy":"2024-04-23T12:08:53.771125Z","iopub.status.idle":"2024-04-23T12:08:53.776262Z","shell.execute_reply":"2024-04-23T12:08:53.775152Z","shell.execute_reply.started":"2024-04-23T12:08:53.771984Z"},"trusted":true},"outputs":[],"source":["# X_test"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-04-23T12:03:24.228668Z","iopub.status.idle":"2024-04-23T12:03:24.229030Z","shell.execute_reply":"2024-04-23T12:03:24.228870Z","shell.execute_reply.started":"2024-04-23T12:03:24.228856Z"},"trusted":true},"outputs":[],"source":["predicted_probabilities = pipeline.predict_proba(X_test)\n","\n","# Define a new threshold\n","new_threshold = 0.45  # Example threshold, you can adjust it according to your needs\n","\n","# Modify predictions based on the new threshold\n","predicted_labels = (predicted_probabilities[:, 1] >= new_threshold).astype(int)\n","\n","# Calculate accuracy\n","accuracy = accuracy_score(Y_test, predicted_labels)\n","print(\"Accuracy:\", accuracy)\n","\n","# Compute confusion matrix\n","cm = confusion_matrix(Y_test, predicted_labels)\n","print(\"Confusion Matrix:\")\n","print(cm)"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-04-23T12:08:56.901765Z","iopub.status.busy":"2024-04-23T12:08:56.901380Z","iopub.status.idle":"2024-04-23T12:08:56.923371Z","shell.execute_reply":"2024-04-23T12:08:56.922416Z","shell.execute_reply.started":"2024-04-23T12:08:56.901735Z"},"trusted":true},"outputs":[{"data":{"text/html":["<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;vectorizer&#x27;,\n","                 TfidfVectorizer(max_df=0.85, max_features=200000,\n","                                 ngram_range=(1, 3))),\n","                (&#x27;classifier&#x27;, LogisticRegression(max_iter=5000))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;vectorizer&#x27;,\n","                 TfidfVectorizer(max_df=0.85, max_features=200000,\n","                                 ngram_range=(1, 3))),\n","                (&#x27;classifier&#x27;, LogisticRegression(max_iter=5000))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(max_df=0.85, max_features=200000, ngram_range=(1, 3))</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=5000)</pre></div></div></div></div></div></div></div>"],"text/plain":["Pipeline(steps=[('vectorizer',\n","                 TfidfVectorizer(max_df=0.85, max_features=200000,\n","                                 ngram_range=(1, 3))),\n","                ('classifier', LogisticRegression(max_iter=5000))])"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["pipeline\n"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-04-23T12:08:57.582076Z","iopub.status.busy":"2024-04-23T12:08:57.581670Z","iopub.status.idle":"2024-04-23T12:08:57.589047Z","shell.execute_reply":"2024-04-23T12:08:57.588002Z","shell.execute_reply.started":"2024-04-23T12:08:57.582042Z"},"trusted":true},"outputs":[{"data":{"text/plain":["3.879674124709792"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["weights[\"crap\"]"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-04-23T12:09:01.011897Z","iopub.status.busy":"2024-04-23T12:09:01.011031Z","iopub.status.idle":"2024-04-23T12:09:08.245798Z","shell.execute_reply":"2024-04-23T12:09:08.244735Z","shell.execute_reply.started":"2024-04-23T12:09:01.011867Z"},"trusted":true},"outputs":[],"source":["X_test_transformed = [replace_with_pos_tags(X_test[i], weights, not(Y_test[i])) for i  in range(len(X_test))]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-22T04:00:42.063884Z","iopub.status.busy":"2024-04-22T04:00:42.063043Z","iopub.status.idle":"2024-04-22T04:00:42.067615Z","shell.execute_reply":"2024-04-22T04:00:42.066663Z","shell.execute_reply.started":"2024-04-22T04:00:42.063847Z"},"trusted":true},"outputs":[],"source":["# X_test_transformed"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-04-23T12:09:10.187578Z","iopub.status.busy":"2024-04-23T12:09:10.187234Z","iopub.status.idle":"2024-04-23T12:09:24.356695Z","shell.execute_reply":"2024-04-23T12:09:24.355453Z","shell.execute_reply.started":"2024-04-23T12:09:10.187553Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting evaluate\n","  Downloading evaluate-0.4.1-py3-none-any.whl.metadata (9.4 kB)\n","Requirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.18.0)\n","Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.26.4)\n","Requirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.8)\n","Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.1.4)\n","Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.31.0)\n","Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate) (4.66.1)\n","Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.4.1)\n","Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.16)\n","Requirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.2.0)\n","Requirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.22.2)\n","Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate) (21.3)\n","Collecting responses<0.19 (from evaluate)\n","  Downloading responses-0.18.0-py3-none-any.whl.metadata (29 kB)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.13.1)\n","Requirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (15.0.2)\n","Requirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (0.6)\n","Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.9.1)\n","Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (6.0.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.9.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->evaluate) (3.1.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.18)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2024.2.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.3.post1)\n","Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.4)\n","Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.3)\n","Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n","Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n","Downloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n","\u001b[?25hDownloading responses-0.18.0-py3-none-any.whl (38 kB)\n","Installing collected packages: responses, evaluate\n","Successfully installed evaluate-0.4.1 responses-0.18.0\n"]}],"source":["!pip install evaluate\n"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-04-23T12:09:24.359298Z","iopub.status.busy":"2024-04-23T12:09:24.358955Z","iopub.status.idle":"2024-04-23T12:09:40.613893Z","shell.execute_reply":"2024-04-23T12:09:40.612632Z","shell.execute_reply.started":"2024-04-23T12:09:24.359268Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting rouge_score\n","  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25ldone\n","\u001b[?25hRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.4.0)\n","Requirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from rouge_score) (3.2.4)\n","Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.26.4)\n","Requirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.16.0)\n","Building wheels for collected packages: rouge_score\n","  Building wheel for rouge_score (setup.py) ... \u001b[?25ldone\n","\u001b[?25h  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=3e91e6b7d028f952323eac9b05f3264c8a5fcbc363f447a35274a94d0856a004\n","  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n","Successfully built rouge_score\n","Installing collected packages: rouge_score\n","Successfully installed rouge_score-0.1.2\n"]}],"source":["from evaluate import load\n","!pip install rouge_score"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-04-23T12:09:40.616031Z","iopub.status.busy":"2024-04-23T12:09:40.615630Z","iopub.status.idle":"2024-04-23T12:09:41.239699Z","shell.execute_reply":"2024-04-23T12:09:41.238687Z","shell.execute_reply.started":"2024-04-23T12:09:40.615991Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0c16c1452e204b0f8897301a7f2bf1ce","version_major":2,"version_minor":0},"text/plain":["Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["metric1 = load(\"rouge\")"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-04-23T12:09:41.242230Z","iopub.status.busy":"2024-04-23T12:09:41.241886Z","iopub.status.idle":"2024-04-23T12:09:42.359135Z","shell.execute_reply":"2024-04-23T12:09:42.358085Z","shell.execute_reply.started":"2024-04-23T12:09:41.242204Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5989fbfa73ad442187d5fb9b6e88f793","version_major":2,"version_minor":0},"text/plain":["Downloading builder script:   0%|          | 0.00/5.94k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"117640e226414fce8b6fb0015c60aff5","version_major":2,"version_minor":0},"text/plain":["Downloading extra modules:   0%|          | 0.00/1.55k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e9e46d8d24cc4edc823f4f9a8eaf27ae","version_major":2,"version_minor":0},"text/plain":["Downloading extra modules:   0%|          | 0.00/3.34k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["metric2 = load(\"bleu\")"]},{"cell_type":"code","execution_count":65,"metadata":{"execution":{"iopub.execute_input":"2024-04-23T13:00:15.829194Z","iopub.status.busy":"2024-04-23T13:00:15.828789Z","iopub.status.idle":"2024-04-23T13:00:16.945819Z","shell.execute_reply":"2024-04-23T13:00:16.944531Z","shell.execute_reply.started":"2024-04-23T13:00:15.829161Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1cf25fe413bc47c083d33b3f95195b55","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"661ac2c8e2c4488297704de04f6ddfb7","version_major":2,"version_minor":0},"text/plain":["spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f1d6ceeb2cfc4ccdaf78384493983756","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["from transformers import AutoTokenizer\n","\n","tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2024-04-23T12:09:43.351247Z","iopub.status.busy":"2024-04-23T12:09:43.350923Z","iopub.status.idle":"2024-04-23T12:09:43.455914Z","shell.execute_reply":"2024-04-23T12:09:43.454713Z","shell.execute_reply.started":"2024-04-23T12:09:43.351219Z"},"trusted":true},"outputs":[],"source":["def read_data(file_path):\n","    with open(file_path, \"r\", encoding=\"latin-1\") as file:\n","        sentences = file.readlines()\n","    return [sentence.strip() for sentence in sentences]\n","\n","\n","# Paths to data files\n","train_formal_path_1 = \"/kaggle/input/Entertainment_Music/train/formal\"\n","train_informal_path_1 = \"/kaggle/input/Entertainment_Music/train/informal\"\n","test_formal_path_1 = \"/kaggle/input/Entertainment_Music/test/formal\"\n","test_informal_path_1 = \"/kaggle/input/Entertainment_Music/test/informal\"\n","\n","train_formal_path_2 = \"/kaggle/input/Family_Relationships/train/formal\"\n","train_informal_path_2 = \"/kaggle/input/Family_Relationships/train/informal\"\n","test_formal_path_2 = \"/kaggle/input/Family_Relationships/test/formal\"\n","test_informal_path_2 = \"/kaggle/input/Family_Relationships/test/informal\"\n","\n","X_train_formal = (read_data(train_formal_path_1) + read_data(train_formal_path_2) )\n","X_train_informal = read_data(train_informal_path_1) + read_data(train_informal_path_2)\n","\n","\n","X_test_formal = read_data(test_formal_path_1) + read_data(test_formal_path_2)\n","X_test_informal = read_data(test_informal_path_1) + read_data(test_informal_path_2)\n"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2024-04-23T12:09:43.457541Z","iopub.status.busy":"2024-04-23T12:09:43.457222Z","iopub.status.idle":"2024-04-23T12:09:43.467741Z","shell.execute_reply":"2024-04-23T12:09:43.466610Z","shell.execute_reply.started":"2024-04-23T12:09:43.457515Z"},"trusted":true},"outputs":[],"source":["X_train_informal_transformed=X_train_transformed[len(X_train)//2:]"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2024-04-23T12:09:51.947435Z","iopub.status.busy":"2024-04-23T12:09:51.946711Z","iopub.status.idle":"2024-04-23T12:09:51.952289Z","shell.execute_reply":"2024-04-23T12:09:51.951065Z","shell.execute_reply.started":"2024-04-23T12:09:51.947402Z"},"trusted":true},"outputs":[],"source":["X_test_informal_transformed=X_test_transformed[len(X_test_formal):]"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2024-04-23T12:10:13.458435Z","iopub.status.busy":"2024-04-23T12:10:13.457738Z","iopub.status.idle":"2024-04-23T12:10:13.484486Z","shell.execute_reply":"2024-04-23T12:10:13.483466Z","shell.execute_reply.started":"2024-04-23T12:10:13.458405Z"},"trusted":true},"outputs":[],"source":["train_informal_df = pd.DataFrame({ 'POS': X_train_informal_transformed, 'informal': X_train_informal,})"]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2024-04-23T12:10:26.686552Z","iopub.status.busy":"2024-04-23T12:10:26.686180Z","iopub.status.idle":"2024-04-23T12:10:26.713182Z","shell.execute_reply":"2024-04-23T12:10:26.712163Z","shell.execute_reply.started":"2024-04-23T12:10:26.686527Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>POS</th>\n","      <th>informal</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>the NN The In-Laws not exactly a holiday NN bu...</td>\n","      <td>the movie The In-Laws not exactly a holiday mo...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>that page did not VB me viroses ( i VBP )</td>\n","      <td>that page did not give me viroses(i think)</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>of NN i be wachin it evry NN , my JJ charachte...</td>\n","      <td>of corse i be wachin it evry day, my fav chara...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>runescape.com ( my NNS VB it ) &amp; funbrain.com ...</td>\n","      <td>runescape.com (my kids love it) &amp; funbrain.com...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Is he RB ? He was on Late Night with Conan O'B...</td>\n","      <td>Is he gay?He was on Late Night with Conan O'Br...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>104557</th>\n","      <td>of NN it VBZ on what NN u r looking for .</td>\n","      <td>of corse it depends on what relation u r looki...</td>\n","    </tr>\n","    <tr>\n","      <th>104558</th>\n","      <td>Wear a sign that says NNP !</td>\n","      <td>Wear a sign that says Hi!</td>\n","    </tr>\n","    <tr>\n","      <th>104559</th>\n","      <td>I do n't VB in VBG games , I VBP when NNS do t...</td>\n","      <td>I don't believe in playing games, I hate when ...</td>\n","    </tr>\n","    <tr>\n","      <th>104560</th>\n","      <td>( or w/e ) p.s NN how JJ r u ?</td>\n","      <td>(or w/e)   p.s gurl how old r u ?</td>\n","    </tr>\n","    <tr>\n","      <th>104561</th>\n","      <td>Try to watch her to VB what NN of NNS she VBZ ...</td>\n","      <td>Try to watch her to see what kind of things sh...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>104562 rows × 2 columns</p>\n","</div>"],"text/plain":["                                                      POS  \\\n","0       the NN The In-Laws not exactly a holiday NN bu...   \n","1               that page did not VB me viroses ( i VBP )   \n","2       of NN i be wachin it evry NN , my JJ charachte...   \n","3       runescape.com ( my NNS VB it ) & funbrain.com ...   \n","4       Is he RB ? He was on Late Night with Conan O'B...   \n","...                                                   ...   \n","104557          of NN it VBZ on what NN u r looking for .   \n","104558                        Wear a sign that says NNP !   \n","104559  I do n't VB in VBG games , I VBP when NNS do t...   \n","104560                     ( or w/e ) p.s NN how JJ r u ?   \n","104561  Try to watch her to VB what NN of NNS she VBZ ...   \n","\n","                                                 informal  \n","0       the movie The In-Laws not exactly a holiday mo...  \n","1              that page did not give me viroses(i think)  \n","2       of corse i be wachin it evry day, my fav chara...  \n","3       runescape.com (my kids love it) & funbrain.com...  \n","4       Is he gay?He was on Late Night with Conan O'Br...  \n","...                                                   ...  \n","104557  of corse it depends on what relation u r looki...  \n","104558                          Wear a sign that says Hi!  \n","104559  I don't believe in playing games, I hate when ...  \n","104560                  (or w/e)   p.s gurl how old r u ?  \n","104561  Try to watch her to see what kind of things sh...  \n","\n","[104562 rows x 2 columns]"]},"execution_count":27,"metadata":{},"output_type":"execute_result"}],"source":["train_informal_df"]},{"cell_type":"code","execution_count":34,"metadata":{"execution":{"iopub.execute_input":"2024-04-23T12:11:12.956894Z","iopub.status.busy":"2024-04-23T12:11:12.956013Z","iopub.status.idle":"2024-04-23T12:11:12.962178Z","shell.execute_reply":"2024-04-23T12:11:12.961297Z","shell.execute_reply.started":"2024-04-23T12:11:12.956861Z"},"trusted":true},"outputs":[],"source":["test_informal_df = pd.DataFrame({ 'POS': X_test_informal_transformed, 'informal': X_test_informal,})"]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2024-04-23T12:10:49.852408Z","iopub.status.busy":"2024-04-23T12:10:49.851421Z","iopub.status.idle":"2024-04-23T12:10:49.864324Z","shell.execute_reply":"2024-04-23T12:10:49.863355Z","shell.execute_reply.started":"2024-04-23T12:10:49.852363Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>POS</th>\n","      <th>informal</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Is Any Baby Really A NNP .</td>\n","      <td>Is Any Baby Really A Freak.</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>aspen NN has he JJS NN NNS , you sit all over ...</td>\n","      <td>aspen colorado has he best music festivals, yo...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>You can VB almost NN on ebay !</td>\n","      <td>You can get almost anything on ebay!</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>everybody is Dying to VB in</td>\n","      <td>everybody is Dying to get in</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>not idiots IN 50 cent and his JJ NNP unit.thos...</td>\n","      <td>not idiots like 50 cent and his whole Gay unit...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>2743</th>\n","      <td>NNS have a problem with secrets .</td>\n","      <td>guys have a problem with secrets.</td>\n","    </tr>\n","    <tr>\n","      <th>2744</th>\n","      <td>I had someone NN JJ with me because I was stil...</td>\n","      <td>I had someone get nasty with me because I was ...</td>\n","    </tr>\n","    <tr>\n","      <th>2745</th>\n","      <td>So how do i just VB him</td>\n","      <td>So how do i just kiss him</td>\n","    </tr>\n","    <tr>\n","      <th>2746</th>\n","      <td>I am the NN of honor for my only NN and I VBP ...</td>\n","      <td>I am the maid of honor for my only sister and ...</td>\n","    </tr>\n","    <tr>\n","      <th>2747</th>\n","      <td>IN it 's with the JJ man .</td>\n","      <td>unless it's with the wrong man.</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>2748 rows × 2 columns</p>\n","</div>"],"text/plain":["                                                    POS  \\\n","0                            Is Any Baby Really A NNP .   \n","1     aspen NN has he JJS NN NNS , you sit all over ...   \n","2                        You can VB almost NN on ebay !   \n","3                           everybody is Dying to VB in   \n","4     not idiots IN 50 cent and his JJ NNP unit.thos...   \n","...                                                 ...   \n","2743                  NNS have a problem with secrets .   \n","2744  I had someone NN JJ with me because I was stil...   \n","2745                            So how do i just VB him   \n","2746  I am the NN of honor for my only NN and I VBP ...   \n","2747                         IN it 's with the JJ man .   \n","\n","                                               informal  \n","0                           Is Any Baby Really A Freak.  \n","1     aspen colorado has he best music festivals, yo...  \n","2                  You can get almost anything on ebay!  \n","3                          everybody is Dying to get in  \n","4     not idiots like 50 cent and his whole Gay unit...  \n","...                                                 ...  \n","2743                  guys have a problem with secrets.  \n","2744  I had someone get nasty with me because I was ...  \n","2745                          So how do i just kiss him  \n","2746  I am the maid of honor for my only sister and ...  \n","2747                    unless it's with the wrong man.  \n","\n","[2748 rows x 2 columns]"]},"execution_count":29,"metadata":{},"output_type":"execute_result"}],"source":["test_formal_df"]},{"cell_type":"code","execution_count":32,"metadata":{"execution":{"iopub.execute_input":"2024-04-23T12:11:05.298349Z","iopub.status.busy":"2024-04-23T12:11:05.297201Z","iopub.status.idle":"2024-04-23T12:11:05.310426Z","shell.execute_reply":"2024-04-23T12:11:05.309367Z","shell.execute_reply.started":"2024-04-23T12:11:05.298310Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>POS</th>\n","      <th>informal</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>the NN The In-Laws not exactly a holiday NN bu...</td>\n","      <td>the movie The In-Laws not exactly a holiday mo...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>that page did not VB me viroses ( i VBP )</td>\n","      <td>that page did not give me viroses(i think)</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>of NN i be wachin it evry NN , my JJ charachte...</td>\n","      <td>of corse i be wachin it evry day, my fav chara...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>runescape.com ( my NNS VB it ) &amp; funbrain.com ...</td>\n","      <td>runescape.com (my kids love it) &amp; funbrain.com...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Is he RB ? He was on Late Night with Conan O'B...</td>\n","      <td>Is he gay?He was on Late Night with Conan O'Br...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>104557</th>\n","      <td>of NN it VBZ on what NN u r looking for .</td>\n","      <td>of corse it depends on what relation u r looki...</td>\n","    </tr>\n","    <tr>\n","      <th>104558</th>\n","      <td>Wear a sign that says NNP !</td>\n","      <td>Wear a sign that says Hi!</td>\n","    </tr>\n","    <tr>\n","      <th>104559</th>\n","      <td>I do n't VB in VBG games , I VBP when NNS do t...</td>\n","      <td>I don't believe in playing games, I hate when ...</td>\n","    </tr>\n","    <tr>\n","      <th>104560</th>\n","      <td>( or w/e ) p.s NN how JJ r u ?</td>\n","      <td>(or w/e)   p.s gurl how old r u ?</td>\n","    </tr>\n","    <tr>\n","      <th>104561</th>\n","      <td>Try to watch her to VB what NN of NNS she VBZ ...</td>\n","      <td>Try to watch her to see what kind of things sh...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>104562 rows × 2 columns</p>\n","</div>"],"text/plain":["                                                      POS  \\\n","0       the NN The In-Laws not exactly a holiday NN bu...   \n","1               that page did not VB me viroses ( i VBP )   \n","2       of NN i be wachin it evry NN , my JJ charachte...   \n","3       runescape.com ( my NNS VB it ) & funbrain.com ...   \n","4       Is he RB ? He was on Late Night with Conan O'B...   \n","...                                                   ...   \n","104557          of NN it VBZ on what NN u r looking for .   \n","104558                        Wear a sign that says NNP !   \n","104559  I do n't VB in VBG games , I VBP when NNS do t...   \n","104560                     ( or w/e ) p.s NN how JJ r u ?   \n","104561  Try to watch her to VB what NN of NNS she VBZ ...   \n","\n","                                                 informal  \n","0       the movie The In-Laws not exactly a holiday mo...  \n","1              that page did not give me viroses(i think)  \n","2       of corse i be wachin it evry day, my fav chara...  \n","3       runescape.com (my kids love it) & funbrain.com...  \n","4       Is he gay?He was on Late Night with Conan O'Br...  \n","...                                                   ...  \n","104557  of corse it depends on what relation u r looki...  \n","104558                          Wear a sign that says Hi!  \n","104559  I don't believe in playing games, I hate when ...  \n","104560                  (or w/e)   p.s gurl how old r u ?  \n","104561  Try to watch her to see what kind of things sh...  \n","\n","[104562 rows x 2 columns]"]},"execution_count":32,"metadata":{},"output_type":"execute_result"}],"source":["train_informal_df"]},{"cell_type":"code","execution_count":35,"metadata":{"execution":{"iopub.execute_input":"2024-04-23T12:11:17.647045Z","iopub.status.busy":"2024-04-23T12:11:17.645976Z","iopub.status.idle":"2024-04-23T12:11:17.658335Z","shell.execute_reply":"2024-04-23T12:11:17.657405Z","shell.execute_reply.started":"2024-04-23T12:11:17.647011Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>POS</th>\n","      <th>informal</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Is Any Baby Really A NNP .</td>\n","      <td>Is Any Baby Really A Freak.</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>aspen NN has he JJS NN NNS , you sit all over ...</td>\n","      <td>aspen colorado has he best music festivals, yo...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>You can VB almost NN on ebay !</td>\n","      <td>You can get almost anything on ebay!</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>everybody is Dying to VB in</td>\n","      <td>everybody is Dying to get in</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>not idiots IN 50 cent and his JJ NNP unit.thos...</td>\n","      <td>not idiots like 50 cent and his whole Gay unit...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>2743</th>\n","      <td>NNS have a problem with secrets .</td>\n","      <td>guys have a problem with secrets.</td>\n","    </tr>\n","    <tr>\n","      <th>2744</th>\n","      <td>I had someone NN JJ with me because I was stil...</td>\n","      <td>I had someone get nasty with me because I was ...</td>\n","    </tr>\n","    <tr>\n","      <th>2745</th>\n","      <td>So how do i just VB him</td>\n","      <td>So how do i just kiss him</td>\n","    </tr>\n","    <tr>\n","      <th>2746</th>\n","      <td>I am the NN of honor for my only NN and I VBP ...</td>\n","      <td>I am the maid of honor for my only sister and ...</td>\n","    </tr>\n","    <tr>\n","      <th>2747</th>\n","      <td>IN it 's with the JJ man .</td>\n","      <td>unless it's with the wrong man.</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>2748 rows × 2 columns</p>\n","</div>"],"text/plain":["                                                    POS  \\\n","0                            Is Any Baby Really A NNP .   \n","1     aspen NN has he JJS NN NNS , you sit all over ...   \n","2                        You can VB almost NN on ebay !   \n","3                           everybody is Dying to VB in   \n","4     not idiots IN 50 cent and his JJ NNP unit.thos...   \n","...                                                 ...   \n","2743                  NNS have a problem with secrets .   \n","2744  I had someone NN JJ with me because I was stil...   \n","2745                            So how do i just VB him   \n","2746  I am the NN of honor for my only NN and I VBP ...   \n","2747                         IN it 's with the JJ man .   \n","\n","                                               informal  \n","0                           Is Any Baby Really A Freak.  \n","1     aspen colorado has he best music festivals, yo...  \n","2                  You can get almost anything on ebay!  \n","3                          everybody is Dying to get in  \n","4     not idiots like 50 cent and his whole Gay unit...  \n","...                                                 ...  \n","2743                  guys have a problem with secrets.  \n","2744  I had someone get nasty with me because I was ...  \n","2745                          So how do i just kiss him  \n","2746  I am the maid of honor for my only sister and ...  \n","2747                    unless it's with the wrong man.  \n","\n","[2748 rows x 2 columns]"]},"execution_count":35,"metadata":{},"output_type":"execute_result"}],"source":["test_informal_df"]},{"cell_type":"code","execution_count":67,"metadata":{"execution":{"iopub.execute_input":"2024-04-23T13:00:56.799503Z","iopub.status.busy":"2024-04-23T13:00:56.799098Z","iopub.status.idle":"2024-04-23T13:01:25.724519Z","shell.execute_reply":"2024-04-23T13:01:25.723354Z","shell.execute_reply.started":"2024-04-23T13:00:56.799472Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","\n","# Assuming you already have X_train_formal_transformed and X_train_formal defined\n","train_informal_df = pd.DataFrame({'POS': X_train_informal_transformed, 'informal': X_train_informal})\n","# train_informal_df = pd.DataFrame({ 'POS': X_train_informal_transformed, 'informal': X_train_informal,})\n","\n","def preprocess_function(row):\n","    inputs = row[\"POS\"] + \"</s>\"\n","    model_inputs = tokenizer(inputs, truncation=True)\n","\n","    # Setup the tokenizer for targets\n","    labels = tokenizer(text_target=row[\"informal\"], truncation=True)\n","\n","    model_inputs[\"labels\"] = labels[\"input_ids\"]\n","    return model_inputs\n","\n","tokenized_dataset_informal_train = train_informal_df.apply(preprocess_function, axis=1)\n","tokenized_dataset_informal_test = test_informal_df.apply(preprocess_function, axis=1)\n"]},{"cell_type":"code","execution_count":68,"metadata":{"execution":{"iopub.execute_input":"2024-04-23T13:01:25.726910Z","iopub.status.busy":"2024-04-23T13:01:25.726503Z","iopub.status.idle":"2024-04-23T13:01:25.734002Z","shell.execute_reply":"2024-04-23T13:01:25.733026Z","shell.execute_reply.started":"2024-04-23T13:01:25.726871Z"},"trusted":true},"outputs":[],"source":["from transformers import AutoModelForSeq2SeqLM\n"]},{"cell_type":"code","execution_count":69,"metadata":{"execution":{"iopub.execute_input":"2024-04-23T13:01:25.735516Z","iopub.status.busy":"2024-04-23T13:01:25.735190Z","iopub.status.idle":"2024-04-23T13:01:25.756558Z","shell.execute_reply":"2024-04-23T13:01:25.755628Z","shell.execute_reply.started":"2024-04-23T13:01:25.735481Z"},"trusted":true},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","train_data, val_data = train_test_split(tokenized_dataset_informal_train, test_size=0.2, random_state=42)\n"]},{"cell_type":"code","execution_count":70,"metadata":{"execution":{"iopub.execute_input":"2024-04-23T13:01:25.759677Z","iopub.status.busy":"2024-04-23T13:01:25.759367Z","iopub.status.idle":"2024-04-23T13:01:25.765435Z","shell.execute_reply":"2024-04-23T13:01:25.763919Z","shell.execute_reply.started":"2024-04-23T13:01:25.759652Z"},"trusted":true},"outputs":[],"source":["data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"]},{"cell_type":"code","execution_count":72,"metadata":{"execution":{"iopub.execute_input":"2024-04-23T13:01:25.780102Z","iopub.status.busy":"2024-04-23T13:01:25.779733Z","iopub.status.idle":"2024-04-23T13:01:25.795961Z","shell.execute_reply":"2024-04-23T13:01:25.794961Z","shell.execute_reply.started":"2024-04-23T13:01:25.780075Z"},"trusted":true},"outputs":[],"source":["import nltk\n","import numpy as np\n","def compute_metrics(eval_pred):\n","    predictions, labels = eval_pred\n","    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n","    preds=[apply_stemming(sentence) for sentence in  decoded_preds]\n","\n","    # Use logistic regression model to predict formality of decoded sentences\n","    predicted_formality = pipeline.predict(preds)\n","\n","    # Calculate accuracy based on logistic regression model's predictions\n","    total_samples = len(decoded_preds)\n","    correct_predictions = sum(predicted_formality == 1)  # Count how many were predicted as formal (0)\n","    accuracy_log_reg = correct_predictions / total_samples\n","\n","    # Replace -100 in the labels as we can't decode them.\n","    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n","    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n","\n","    # Rouge expects a newline after each sentence\n","    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n","    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n","\n","    # Note that other metrics may not have a `use_aggregator` parameter\n","    # and thus will return a list, computing a metric for each sentence.\n","    result = metric1.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True, use_aggregator=True)\n","    # Extract a few results\n","    result = {key: value * 100 for key, value in result.items()}\n","\n","    # Add mean generated length\n","    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n","    result[\"gen_len\"] = np.mean(prediction_lens)\n","\n","    # Add accuracy based on logistic regression model's predictions\n","    result[\"accuracy_log_reg\"] = accuracy_log_reg\n","    \n","    print(result)\n","\n","    return {k: round(v, 4) for k, v in result.items()}\n"]},{"cell_type":"code","execution_count":73,"metadata":{"execution":{"iopub.execute_input":"2024-04-23T13:01:25.797515Z","iopub.status.busy":"2024-04-23T13:01:25.797213Z","iopub.status.idle":"2024-04-23T13:01:26.192095Z","shell.execute_reply":"2024-04-23T13:01:26.190831Z","shell.execute_reply.started":"2024-04-23T13:01:25.797490Z"},"trusted":true},"outputs":[],"source":["train_dataset = train_data.to_frame().to_dict(orient='records')\n","val_dataset = val_data.to_frame().to_dict(orient='records')"]},{"cell_type":"code","execution_count":74,"metadata":{"execution":{"iopub.execute_input":"2024-04-23T13:01:26.193867Z","iopub.status.busy":"2024-04-23T13:01:26.193502Z","iopub.status.idle":"2024-04-23T13:01:26.253633Z","shell.execute_reply":"2024-04-23T13:01:26.252378Z","shell.execute_reply.started":"2024-04-23T13:01:26.193833Z"},"trusted":true},"outputs":[],"source":["train_dataset1 = [v for d in train_dataset for k, v in d.items()]\n","val_dataset1=[v for d in val_dataset for k, v in d.items()]"]},{"cell_type":"code","execution_count":48,"metadata":{"execution":{"iopub.execute_input":"2024-04-23T12:13:31.546481Z","iopub.status.busy":"2024-04-23T12:13:31.546126Z","iopub.status.idle":"2024-04-23T12:13:31.551870Z","shell.execute_reply":"2024-04-23T12:13:31.550857Z","shell.execute_reply.started":"2024-04-23T12:13:31.546456Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[{'input_ids': [852, 3, 88, 19, 584, 19179, 91, 28, 3, 9, 9269, 3, 17235, 3, 5, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [852, 3, 88, 19, 352, 91, 28, 3, 9, 9269, 4024, 5, 1]}, {'input_ids': [3, 2, 834, 2, 27, 584, 11165, 12, 3, 22086, 14883, 11113, 8, 445, 9082, 28, 82, 446, 683, 19665, 230, 11, 258, 3, 5, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [3, 2, 834, 2, 27, 333, 12, 1605, 14883, 11113, 8, 1908, 32, 107, 28, 82, 385, 19665, 230, 11, 258, 5, 1]}, {'input_ids': [11, 24, 3, 31, 7, 59, 3, 9, 446, 683, 3, 17235, 3, 5, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [11, 24, 31, 7, 59, 3, 9, 1282, 589, 5, 1]}, {'input_ids': [27, 584, 14594, 11790, 15, 63, 3, 88, 3, 31, 7, 78, 5295, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [27, 6528, 11790, 15, 63, 3, 88, 31, 7, 78, 5295, 1]}, {'input_ids': [445, 9082, 3, 6, 3396, 24786, 3, 6, 24686, 9012, 3, 6, 4083, 517, 27514, 3, 6, 22694, 13738, 20152, 17063, 3, 6, 5055, 445, 9082, 445, 9082, 3, 5, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [309, 19056, 17892, 6, 3396, 24786, 6, 24686, 9012, 6, 4083, 517, 27514, 6, 22694, 13738, 20152, 17063, 6, 5055, 3, 9312, 476, 13209, 15355, 476, 11810, 134, 5, 1]}, {'input_ids': [80, 1962, 3, 76, 8530, 36, 1095, 8, 416, 20, 8918, 11, 584, 19179, 12, 5781, 276, 6294, 3, 5, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [80, 1962, 3, 76, 228, 36, 1095, 8, 416, 20, 8918, 11, 7570, 12, 5781, 3, 450, 7703, 5, 1]}, {'input_ids': [445, 9082, 59, 1086, 114, 48, 28, 119, 3567, 3388, 3, 5, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [1318, 59, 1086, 114, 48, 28, 119, 3567, 713, 5, 1]}, {'input_ids': [3, 76, 3, 22086, 140, 13, 24, 3, 17235, 113, 21042, 15, 26, 21, 797, 445, 9082, 446, 683, 774, 3, 233, 54, 25, 9799, 34, 3, 58, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [3, 76, 5607, 140, 13, 24, 4024, 113, 21042, 15, 26, 21, 797, 27, 26, 32, 40, 336, 774, 233, 54, 25, 9799, 34, 58, 1]}, {'input_ids': [1377, 13, 135, 143, 34, 446, 683, 78, 3, 99, 34, 1416, 3388, 3, 88, 3, 22086, 956, 25, 3, 6, 3, 88, 3, 12108, 405, 5, 5, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [1377, 13, 135, 143, 34, 4813, 78, 3, 99, 34, 1416, 114, 3, 88, 114, 7, 25, 6, 3, 88, 1077, 405, 5, 5, 1]}, {'input_ids': [3388, 27, 43, 12, 6264, 27, 43, 3, 9, 3, 17235, 13, 445, 7369, 24, 43, 3, 5, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [4229, 27, 43, 12, 6264, 27, 43, 3, 9, 418, 13, 803, 24, 43, 5, 1]}]\n"]}],"source":["print(train_dataset1[:10])"]},{"cell_type":"code","execution_count":50,"metadata":{"execution":{"iopub.execute_input":"2024-04-23T12:13:59.627503Z","iopub.status.busy":"2024-04-23T12:13:59.626721Z","iopub.status.idle":"2024-04-23T12:13:59.665227Z","shell.execute_reply":"2024-04-23T12:13:59.664160Z","shell.execute_reply.started":"2024-04-23T12:13:59.627470Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3bcb07c7ee2943799015f52271b73e93","version_major":2,"version_minor":0},"text/plain":["VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"]},"metadata":{},"output_type":"display_data"}],"source":["from huggingface_hub import notebook_login\n","\n","notebook_login()"]},{"cell_type":"code","execution_count":75,"metadata":{"execution":{"iopub.execute_input":"2024-04-23T13:01:26.255396Z","iopub.status.busy":"2024-04-23T13:01:26.255088Z","iopub.status.idle":"2024-04-23T13:01:27.409360Z","shell.execute_reply":"2024-04-23T13:01:27.408241Z","shell.execute_reply.started":"2024-04-23T13:01:26.255370Z"},"trusted":true},"outputs":[],"source":["trainer = Seq2SeqTrainer(\n","    model,\n","    args,\n","    train_dataset=train_dataset1 ,\n","    eval_dataset=val_dataset1,\n","    data_collator=data_collator,\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-23T13:02:25.443822Z","iopub.status.busy":"2024-04-23T13:02:25.442993Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='13008' max='26145' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [13008/26145 52:37 < 53:09, 4.12 it/s, Epoch 2.49/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Rouge1</th>\n","      <th>Rouge2</th>\n","      <th>Rougel</th>\n","      <th>Rougelsum</th>\n","      <th>Gen Len</th>\n","      <th>Accuracy Log Reg</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.951300</td>\n","      <td>0.817946</td>\n","      <td>79.305300</td>\n","      <td>64.409000</td>\n","      <td>79.184800</td>\n","      <td>79.179700</td>\n","      <td>14.975000</td>\n","      <td>0.854500</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.864900</td>\n","      <td>0.757735</td>\n","      <td>80.204700</td>\n","      <td>66.004000</td>\n","      <td>80.101400</td>\n","      <td>80.095800</td>\n","      <td>14.971200</td>\n","      <td>0.866500</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1141: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["{'rouge1': 79.30532436342062, 'rouge2': 64.4089665138969, 'rougeL': 79.18476650749918, 'rougeLsum': 79.1797038745427, 'gen_len': 14.975039449146465, 'accuracy_log_reg': 0.8545402381293932}\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1141: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["{'rouge1': 80.20468205422158, 'rouge2': 66.00398643469936, 'rougeL': 80.10139414255583, 'rougeLsum': 80.09578572125537, 'gen_len': 14.971214077368144, 'accuracy_log_reg': 0.8664945249366423}\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","IOPub message rate exceeded.\n","The notebook server will temporarily stop sending output\n","to the client in order to avoid crashing it.\n","To change this limit, set the config variable\n","`--NotebookApp.iopub_msg_rate_limit`.\n","\n","Current values:\n","NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n","NotebookApp.rate_limit_window=3.0 (secs)\n","\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n"]}],"source":["trainer.train()"]},{"cell_type":"code","execution_count":86,"metadata":{"execution":{"iopub.execute_input":"2024-04-23T15:43:55.578032Z","iopub.status.busy":"2024-04-23T15:43:55.577538Z","iopub.status.idle":"2024-04-23T15:44:07.053192Z","shell.execute_reply":"2024-04-23T15:44:07.052075Z","shell.execute_reply.started":"2024-04-23T15:43:55.578000Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"25c63cb99dbf41138039b1fe41293028","version_major":2,"version_minor":0},"text/plain":["events.out.tfevents.1713877346.cbf3c2509588.34.3:   0%|          | 0.00/20.0k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2c19b54619c848ce8b209c2e996af60c","version_major":2,"version_minor":0},"text/plain":["Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fcfeec5c52dd4358a83c28957ef0ab8b","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["CommitInfo(commit_url='https://huggingface.co/Pushparaj20/t5-small-informal/commit/27c96c85a91cb399cfc284213d75fa6b7175dff3', commit_message='Pushparaj20/informal_transfer', commit_description='', oid='27c96c85a91cb399cfc284213d75fa6b7175dff3', pr_url=None, pr_revision=None, pr_num=None)"]},"execution_count":86,"metadata":{},"output_type":"execute_result"}],"source":["trainer.push_to_hub(\"Pushparaj20/informal_transfer\")"]},{"cell_type":"code","execution_count":88,"metadata":{"execution":{"iopub.execute_input":"2024-04-23T15:44:27.583861Z","iopub.status.busy":"2024-04-23T15:44:27.583145Z","iopub.status.idle":"2024-04-23T15:44:34.435932Z","shell.execute_reply":"2024-04-23T15:44:34.434811Z","shell.execute_reply.started":"2024-04-23T15:44:27.583829Z"},"trusted":true},"outputs":[{"data":{"text/plain":["['weight.pkl']"]},"execution_count":88,"metadata":{},"output_type":"execute_result"}],"source":["import joblib\n","weights_file = \"weight.pkl\"\n","joblib.dump(weights, weights_file)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-22T09:07:55.766488Z","iopub.status.busy":"2024-04-22T09:07:55.765769Z","iopub.status.idle":"2024-04-22T09:07:55.777655Z","shell.execute_reply":"2024-04-22T09:07:55.776558Z","shell.execute_reply.started":"2024-04-22T09:07:55.766454Z"},"trusted":true},"outputs":[],"source":["import joblib\n","\n","# Save the entire pipeline (including vectorizer and classifier)\n","# joblib.dump(pipeline, 'formality_model.pkl')\n","\n","# Save the logistic regression model weights\n","joblib.dump(pipeline.named_steps['classifier'].coef_, 'weights.pkl')\n"]},{"cell_type":"code","execution_count":137,"metadata":{"execution":{"iopub.execute_input":"2024-04-22T09:38:16.138312Z","iopub.status.busy":"2024-04-22T09:38:16.137915Z","iopub.status.idle":"2024-04-22T09:38:16.743806Z","shell.execute_reply":"2024-04-22T09:38:16.742841Z","shell.execute_reply.started":"2024-04-22T09:38:16.138279Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["File 'weight.pkl' downloaded successfully to 'logreg'.\n"]}],"source":["import os\n","import requests\n","\n","# Define the repository and file names\n","repo_name = \"Pushparaj20/t5-base-finetuned\"  # Replace with your actual username and repository name\n","file_name = \"weight.pkl\"\n","\n","# Construct the URL to fetch the file from the repository\n","file_url = f\"https://huggingface.co/{repo_name}/resolve/main/{file_name}\"\n","\n","# Define the directory where you want to save the file locally\n","output_dir = \"logreg\"  # Choose a directory name\n","\n","# Make a GET request to download the file\n","response = requests.get(file_url)\n","\n","# Check if the request was successful\n","if response.status_code == 200:\n","    # Create the output directory if it doesn't exist\n","    os.makedirs(output_dir, exist_ok=True)\n","    # Write the file content to disk\n","    with open(os.path.join(output_dir, file_name), \"wb\") as file:\n","        file.write(response.content)\n","    print(f\"File '{file_name}' downloaded successfully to '{output_dir}'.\")\n","else:\n","    print(f\"Failed to download '{file_name}' from '{repo_name}'. Status code: {response.status_code}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-21T09:27:41.139520Z","iopub.status.busy":"2024-04-21T09:27:41.138861Z","iopub.status.idle":"2024-04-21T09:27:44.425514Z","shell.execute_reply":"2024-04-21T09:27:44.423901Z","shell.execute_reply.started":"2024-04-21T09:27:41.139490Z"},"trusted":true},"outputs":[],"source":["def replace_with_pos_tags(sentence, weights, feature_names, is_formal):\n","    stemmer = PorterStemmer()\n","    tokens = word_tokenize(sentence)\n","    stemmed_tokens = [stemmer.stem(token) for token in tokens]  # Stem tokens\n","    pos_tags = pos_tag(tokens)  # Use original tokens for POS tagging\n","    replaced_sentence = []\n","\n","    # Calculate N as described\n","    N = len(stemmed_tokens) // 5\n","\n","    # Sort stemmed tokens by weight\n","    sorted_stemmed_tokens = sorted(stemmed_tokens, key=lambda x: abs(weights[0, feature_names.index(x)]), reverse=True)\n","\n","    # Extract N stemmed terms with highest absolute weights\n","    top_N_stemmed_tokens = sorted_stemmed_tokens[:N]\n","\n","    for token, tag in pos_tags:\n","        stemmed_token = stemmer.stem(token)\n","        if stemmed_token in top_N_stemmed_tokens:\n","            weight = abs(weights[0, feature_names.index(stemmed_token)])\n","            if (is_formal and weight >= 0.01) or (not is_formal and weight >= 0.02):\n","                replaced_sentence.append(tag)\n","            else:\n","                replaced_sentence.append(token)\n","        else:\n","            replaced_sentence.append(token)\n","\n","    return ' '.join(replaced_sentence)\n","\n","\n","\n","sentence = \"I don't think that page gave me viruses.\"\n","weights_file = \"logregweights.pkl\"  # Path to the weights file within the logreg directory\n","is_formal = True  # Example value\n","\n","# Load the weights from the file\n","weights = joblib.load(weights_file)\n","\n","# Ensure that you also pass the feature_names argument\n","transformed_sentence = replace_with_pos_tags(sentence, weights, feature_names, is_formal)\n","print(\"Transformed Sentence:\", transformed_sentence)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-21T09:15:59.275585Z","iopub.status.busy":"2024-04-21T09:15:59.274857Z","iopub.status.idle":"2024-04-21T09:15:59.283556Z","shell.execute_reply":"2024-04-21T09:15:59.281952Z","shell.execute_reply.started":"2024-04-21T09:15:59.275547Z"},"trusted":true},"outputs":[],"source":["weights_file\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-22T09:20:03.184764Z","iopub.status.busy":"2024-04-22T09:20:03.184379Z","iopub.status.idle":"2024-04-22T09:20:03.246980Z","shell.execute_reply":"2024-04-22T09:20:03.245560Z","shell.execute_reply.started":"2024-04-22T09:20:03.184733Z"},"trusted":true},"outputs":[],"source":["import joblib\n","\n","# Load the weights from the file\n","loaded_weights = joblib.load('new.pkl')\n","\n","# Print the type and shape of the loaded weights\n","print(\"Type of loaded weights:\", type(loaded_weights))\n","print(\"Shape of loaded weights:\", loaded_weights.shape)\n","\n","# Print the loaded weights\n","print(\"Loaded weights:\", loaded_weights)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-22T09:06:39.774918Z","iopub.status.busy":"2024-04-22T09:06:39.774569Z","iopub.status.idle":"2024-04-22T09:06:39.793392Z","shell.execute_reply":"2024-04-22T09:06:39.792487Z","shell.execute_reply.started":"2024-04-22T09:06:39.774892Z"},"trusted":true},"outputs":[],"source":["pipeline"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-22T09:23:51.366575Z","iopub.status.busy":"2024-04-22T09:23:51.365602Z","iopub.status.idle":"2024-04-22T09:23:54.627835Z","shell.execute_reply":"2024-04-22T09:23:54.626677Z","shell.execute_reply.started":"2024-04-22T09:23:51.366530Z"},"trusted":true},"outputs":[],"source":["import joblib\n","\n","# Load the weights from the file\n","loaded_weights = joblib.load('logreg_weights.pkl')\n","\n","# Print the type of loaded weights and the first few entries\n","print(type(loaded_weights))\n","print(list(loaded_weights.items())[:10])  # Print the first 10 items\n","\n","\n","# Check if the structure is similar to the weights dictionary\n"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-22T09:32:31.766322Z","iopub.status.busy":"2024-04-22T09:32:31.765686Z","iopub.status.idle":"2024-04-22T09:32:34.910359Z","shell.execute_reply":"2024-04-22T09:32:34.909399Z","shell.execute_reply.started":"2024-04-22T09:32:31.766288Z"},"trusted":true},"outputs":[],"source":["import joblib\n","from nltk.tokenize import word_tokenize\n","from nltk import pos_tag\n","from nltk.stem import PorterStemmer\n","\n","# Load the weights from the file\n","weights_file = 'logreg/weight.pkl'\n","loaded_weights = joblib.load(weights_file)\n","\n","# Define a sample sentence\n","sentence = \"s.\"\n","\n","# Specify the value of is_formal\n","is_formal = True  # Example value\n","stemmer = SnowballStemmer(\"english\")\n","\n","# Define the function to replace words with POS tags\n","def replace_with_pos_tags(sentence, weights, is_formal):\n","    tokens = word_tokenize(sentence)\n","    stemmed_tokens = [stemmer.stem(token) for token in tokens]  # Stem tokens\n","    pos_tags = pos_tag(tokens)  # Use original tokens for POS tagging\n","    replaced_sentence = []\n","\n","    # Calculate N as described\n","    N = len(stemmed_tokens) // 4\n","\n","    # Filter out stop words and stem them\n","    tokens_without_stopwords = [stemmer.stem(token) for token in tokens if token.lower() not in stop_words]\n","\n","    # Sort stemmed tokens by weight\n","    sorted_tokens = sorted(tokens_without_stopwords, key=lambda x: abs(weights.get(x, 0)), reverse=True)\n","\n","    # Extract N tokens with highest absolute weights\n","    top_N_tokens = sorted_tokens[:N]\n","\n","    for token, tag in pos_tags:\n","        stemmed_token = stemmer.stem(token)\n","        if stemmed_token in top_N_tokens:\n","            weight = abs(weights.get(stemmed_token, 0))\n","            if (is_formal and weight >= 0.01) or (not is_formal and weight >= 0.02):\n","                replaced_sentence.append(tag)\n","            else:\n","                replaced_sentence.append(token)\n","        else:\n","            replaced_sentence.append(token)\n","\n","    return ' '.join(replaced_sentence)\n","\n","# Call the function with loaded weights, sample sentence, and is_formal value\n","transformed_sentence = replace_with_pos_tags(sentence, loaded_weights, is_formal)\n","\n","# Print the transformed sentence\n","print(\"Transformed Sentence:\", transformed_sentence)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-22T09:25:22.082754Z","iopub.status.busy":"2024-04-22T09:25:22.082367Z","iopub.status.idle":"2024-04-22T09:25:29.018853Z","shell.execute_reply":"2024-04-22T09:25:29.017735Z","shell.execute_reply.started":"2024-04-22T09:25:22.082723Z"},"trusted":true},"outputs":[],"source":["weights_file = \"weight.pkl\"\n","joblib.dump(weights, weights_file)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import os\n","import requests\n","\n","# Define the repository and file names\n","repo_name = \"Pushparaj20/t5-base-finetuned\"  # Replace with your actual username and repository name\n","file_name = \"weight.pkl\"\n","\n","# Construct the URL to fetch the file from the repository\n","file_url = f\"https://huggingface.co/{repo_name}/resolve/main/{file_name}\"\n","\n","# Define the directory where you want to save the file locally\n","output_dir = \"logreg\"  # Choose a directory name\n","\n","# Make a GET request to download the file\n","response = requests.get(file_url)\n","\n","# Check if the request was successful\n","if response.status_code == 200:\n","    # Create the output directory if it doesn't exist\n","    os.makedirs(output_dir, exist_ok=True)\n","    # Write the file content to disk\n","    with open(os.path.join(output_dir, file_name), \"wb\") as file:\n","        file.write(response.content)\n","    print(f\"File '{file_name}' downloaded successfully to '{output_dir}'.\")\n","else:\n","    print(f\"Failed to download '{file_name}' from '{repo_name}'. Status code: {response.status_code}\")\n"]},{"cell_type":"code","execution_count":176,"metadata":{"execution":{"iopub.execute_input":"2024-04-22T09:57:42.035181Z","iopub.status.busy":"2024-04-22T09:57:42.034178Z","iopub.status.idle":"2024-04-22T09:57:44.477862Z","shell.execute_reply":"2024-04-22T09:57:44.476809Z","shell.execute_reply.started":"2024-04-22T09:57:42.035146Z"},"trusted":true},"outputs":[],"source":["from transformers import AutoModelForSeq2SeqLM,AutoTokenizer\n","\n","# Load the model from the model hu\n","model = AutoModelForSeq2SeqLM.from_pretrained(\"Pushparaj20/t5-base-finetuned\")\n","from transformers import AutoTokenizer\n","\n","# Load the tokenizer from the model hub\n","tokenizer = AutoTokenizer.from_pretrained(\"Pushparaj20/t5-base-finetuned\")\n"]},{"cell_type":"code","execution_count":170,"metadata":{"execution":{"iopub.execute_input":"2024-04-22T09:47:10.571094Z","iopub.status.busy":"2024-04-22T09:47:10.570415Z","iopub.status.idle":"2024-04-22T09:47:13.776122Z","shell.execute_reply":"2024-04-22T09:47:13.774878Z","shell.execute_reply.started":"2024-04-22T09:47:10.571062Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Transformed Sentence: NN ! It 's finger-lickin ' JJ .\n"]}],"source":["import joblib\n","from nltk.tokenize import word_tokenize\n","from nltk import pos_tag\n","from nltk.stem import PorterStemmer\n","\n","# Load the weights from the file\n","weights_file = 'logreg/weight.pkl'\n","loaded_weights = joblib.load(weights_file)\n","\n","# Define a sample sentence\n","sentence =  \"OMG! It's finger-lickin' good.\"\n","\n","# Specify the value of is_formal\n","is_formal = True  # Example value\n","stemmer = SnowballStemmer(\"english\")\n","\n","# Define the function to replace words with POS tags\n","def replace_with_pos_tags(sentence, weights, is_formal):\n","    tokens = word_tokenize(sentence)\n","    stemmed_tokens = [stemmer.stem(token) for token in tokens]  # Stem tokens\n","    pos_tags = pos_tag(tokens)  # Use original tokens for POS tagging\n","    replaced_sentence = []\n","\n","    # Calculate N as described\n","    N = len(stemmed_tokens) // 4\n","\n","    # Filter out stop words and stem them\n","    tokens_without_stopwords = [stemmer.stem(token) for token in tokens if token.lower() not in stop_words]\n","\n","    # Sort stemmed tokens by weight\n","    sorted_tokens = sorted(tokens_without_stopwords, key=lambda x: abs(weights.get(x, 0)), reverse=True)\n","\n","    # Extract N tokens with highest absolute weights\n","    top_N_tokens = sorted_tokens[:N]\n","\n","    for token, tag in pos_tags:\n","        stemmed_token = stemmer.stem(token)\n","        if stemmed_token in top_N_tokens:\n","            weight = abs(weights.get(stemmed_token, 0))\n","            if (is_formal and weight >= 0.01) or (not is_formal and weight >= 0.02):\n","                replaced_sentence.append(tag)\n","            else:\n","                replaced_sentence.append(token)\n","        else:\n","            replaced_sentence.append(token)\n","\n","    return ' '.join(replaced_sentence)\n","\n","# Call the function with loaded weights, sample sentence, and is_formal value\n","transformed_sentence = replace_with_pos_tags(sentence, loaded_weights, is_formal)\n","\n","# Print the transformed sentence\n","print(\"Transformed Sentence:\", transformed_sentence)\n"]},{"cell_type":"code","execution_count":177,"metadata":{"execution":{"iopub.execute_input":"2024-04-22T09:57:45.364817Z","iopub.status.busy":"2024-04-22T09:57:45.364454Z","iopub.status.idle":"2024-04-22T09:57:45.949046Z","shell.execute_reply":"2024-04-22T09:57:45.947885Z","shell.execute_reply.started":"2024-04-22T09:57:45.364792Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[    3, 17235,     3,    55,    94,     3,    31,     7,  8425,    18,\n","         10095,    77,     3,    31,   446,   683,     3,     5,     1]])\n"]},{"data":{"text/plain":["\"Hey! It's finger-lickin' hot.\""]},"execution_count":177,"metadata":{},"output_type":"execute_result"}],"source":["\n","input_ids = tokenizer(transformed_sentence, return_tensors=\"pt\").input_ids\n","print(input_ids)\n","\n","# Generate predictions\n","output = model.generate(input_ids)\n","b=output.numpy()\n","tokenizer.decode(b[0], skip_special_tokens=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":4742002,"sourceId":8042803,"sourceType":"datasetVersion"}],"dockerImageVersionId":30703,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
